{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業:\n",
    "嘗試調整參數:  \n",
    "sg:sg=1表示採用skip-gram,sg=0 表示採用cbow  \n",
    "window:能往左往右看幾個字的意思 \n",
    "\n",
    "    參數說明\n",
    "    sentences：當然了了，這是要訓練的句句⼦子集，沒有他就不⽤用跑了了 \n",
    "    size：這表⽰示的是訓練出的詞向量量會有幾維 \n",
    "    alpha：機器學習中的學習率，這東⻄西會逐漸收斂到 min_alpha \n",
    "    sg：sg=1表⽰示採⽤用skip-gram，sg=0 表⽰示採⽤用cbow \n",
    "    window：能往左往右看幾個字的意思 \n",
    "    workers：執⾏行行緒數⽬目， \n",
    "    min_count：若若這個詞出現的次數⼩小於min_count，那他就不會被視為訓練對\n",
    "    象\n",
    "    Note: 若若是運⾏行行過程中碰到記憶體不⾜足的問題, 可以把worker 的值設置在 4 以下\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim, logging\n",
    "from gensim.models import word2vec\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 22:04:31,028 : INFO : collecting all words and their counts\n",
      "2019-03-15 22:04:31,032 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-03-15 22:04:31,032 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2019-03-15 22:04:31,032 : INFO : Loading a fresh vocabulary\n",
      "2019-03-15 22:04:31,036 : INFO : min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2019-03-15 22:04:31,036 : INFO : min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2019-03-15 22:04:31,036 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2019-03-15 22:04:31,040 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2019-03-15 22:04:31,044 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2019-03-15 22:04:31,044 : INFO : estimated required memory for 3 words and 256 dimensions: 7644 bytes\n",
      "2019-03-15 22:04:31,044 : INFO : resetting layer weights\n",
      "2019-03-15 22:04:31,044 : INFO : training model with 4 workers on 3 vocabulary and 256 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-03-15 22:04:31,048 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 22:04:31,052 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 22:04:31,052 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 22:04:31,052 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 22:04:31,052 : INFO : EPOCH - 1 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-03-15 22:04:31,072 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 22:04:31,072 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 22:04:31,076 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 22:04:31,076 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 22:04:31,080 : INFO : EPOCH - 2 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-03-15 22:04:31,084 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 22:04:31,084 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 22:04:31,092 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 22:04:31,100 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 22:04:31,104 : INFO : EPOCH - 3 : training on 4 raw words (1 effective words) took 0.0s, 52 effective words/s\n",
      "2019-03-15 22:04:31,112 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 22:04:31,116 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 22:04:31,116 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 22:04:31,120 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 22:04:31,120 : INFO : EPOCH - 4 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-03-15 22:04:31,132 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 22:04:31,132 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 22:04:31,132 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 22:04:31,136 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 22:04:31,136 : INFO : EPOCH - 5 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-03-15 22:04:31,140 : INFO : training on a 20 raw words (1 effective words) took 0.1s, 11 effective words/s\n",
      "2019-03-15 22:04:31,140 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) \n",
    "sentences = [['first', 'sentence'], ['second', 'sentence']]  \n",
    "model = word2vec.Word2Vec(sentences, size=256, min_count=1, window=5, workers=4, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=3, size=256, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09817103820984503"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('first','second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 22:04:31,224 : INFO : saving Word2Vec object under mymodel, separately None\n",
      "2019-03-15 22:04:31,224 : INFO : not storing attribute vectors_norm\n",
      "2019-03-15 22:04:31,232 : INFO : not storing attribute cum_table\n",
      "2019-03-15 22:04:31,252 : INFO : saved mymodel\n",
      "2019-03-15 22:04:31,256 : INFO : loading Word2Vec object from mymodel\n",
      "2019-03-15 22:04:31,256 : INFO : loading wv recursively from mymodel.wv.* with mmap=None\n",
      "2019-03-15 22:04:31,260 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-03-15 22:04:31,268 : INFO : loading vocabulary recursively from mymodel.vocabulary.* with mmap=None\n",
      "2019-03-15 22:04:31,272 : INFO : loading trainables recursively from mymodel.trainables.* with mmap=None\n",
      "2019-03-15 22:04:31,272 : INFO : setting ignored attribute cum_table to None\n",
      "2019-03-15 22:04:31,272 : INFO : loaded mymodel\n"
     ]
    }
   ],
   "source": [
    "model.save('mymodel')  \n",
    "new_model = gensim.models.Word2Vec.load('mymodel')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
